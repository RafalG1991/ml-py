{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdb26d84",
   "metadata": {},
   "source": [
    "# Laboratory 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7069b0c",
   "metadata": {},
   "source": [
    "## Description of the `IMDB` dataset\n",
    "The `IMDB` dataset is a set of 50,000 highly polarized reviews from the Internet Movie Database. They’re split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting of 50% negative and 50% positive reviews. The reviews (sequences of words) have been preprocessed - turned into sequences of integers, where each integer stands for a specific word in a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc967e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Programy\\Anaconda\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
    "    num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a80ff1f",
   "metadata": {},
   "source": [
    "The argument `num_words=10000` means we’ll only keep the top 10,000 most frequently occurring words in the training data. Rare words will be discarded. The variables `train_data` and `test_data` are lists of reviews; each review is a list of word indices (encoding a sequence of words). `train_labels` and `test_labels` are lists of 0s and 1s, where 0 stands for negative and 1 stands for positive. For instance, the first review consists of 218 words and is positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24df61f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   1,   14,   22,   16,   43,  530,  973, 1622, 1385,   65,  458,\n",
       "        4468,   66, 3941,    4,  173,   36,  256,    5,   25,  100,   43,\n",
       "         838,  112,   50,  670,    2,    9,   35,  480,  284,    5,  150,\n",
       "           4,  172,  112,  167,    2,  336,  385,   39,    4,  172, 4536,\n",
       "        1111,   17,  546,   38,   13,  447,    4,  192,   50,   16,    6,\n",
       "         147, 2025,   19,   14,   22,    4, 1920, 4613,  469,    4,   22,\n",
       "          71,   87,   12,   16,   43,  530,   38,   76,   15,   13, 1247,\n",
       "           4,   22,   17,  515,   17,   12,   16,  626,   18,    2,    5,\n",
       "          62,  386,   12,    8,  316,    8,  106,    5,    4, 2223, 5244,\n",
       "          16,  480,   66, 3785,   33,    4,  130,   12,   16,   38,  619,\n",
       "           5,   25,  124,   51,   36,  135,   48,   25, 1415,   33,    6,\n",
       "          22,   12,  215,   28,   77,   52,    5,   14,  407,   16,   82,\n",
       "           2,    8,    4,  107,  117, 5952,   15,  256,    4,    2,    7,\n",
       "        3766,    5,  723,   36,   71,   43,  530,  476,   26,  400,  317,\n",
       "          46,    7,    4,    2, 1029,   13,  104,   88,    4,  381,   15,\n",
       "         297,   98,   32, 2071,   56,   26,  141,    6,  194, 7486,   18,\n",
       "           4,  226,   22,   21,  134,  476,   26,  480,    5,  144,   30,\n",
       "        5535,   18,   51,   36,   28,  224,   92,   25,  104,    4,  226,\n",
       "          65,   16,   38, 1334,   88,   12,   16,  283,    5,   16, 4472,\n",
       "         113,  103,   32,   15,   16, 5345,   19,  178,   32]),\n",
       " 218,\n",
       " 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(train_data[0]),len(train_data[0]),train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d27670",
   "metadata": {},
   "source": [
    "We can easily decode any of these reviews back to English words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "933e57d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce777937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoded_review(number_of_review):\n",
    "    return \" \".join([reverse_word_index.get(i - 3, \"?\") for i in train_data[number_of_review]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3509c982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? although i had seen ? in a theater way back in ? i couldn't remember anything of the plot except for vague images of kurt thomas running and fighting against a backdrop of stone walls and disappointment regarding the ending br br after reading some of the other reviews i picked up a copy of the newly released dvd to once again enter the world of ? br br it turns out this is one of those films produced during the '80s that would go directly to video today the film stars champion ? kurt thomas as jonathan ? ? out of the blue to ? the nation of ? to enter and hopefully win the game a ? ? ? by the khan who encourages his people by yelling what sounds like ? power the goal of the mission involves the star wars defense system jonathan is trained in the martial arts by princess ? who never speaks or leaves the house once trained tries to blend in with the locals by wearing a bright red ? with ? of blue and white needless to say ? finds himself running and fighting for his life along the stone streets of ? on his way to a date with destiny and the game br br star kurt thomas was ill served by director robert ? who it looks like was never on the set the so called script is just this side of incompetent see other reviews for the many ? throughout the town of ? has a few good moments but is ultimately ruined by bad editing the ending ? still there's the ? of a good action adventure here a hong kong version with more visceral action and faster pace might even be pretty good\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_review = 1000\n",
    "decoded_review(number_of_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6289d2a8",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "Prepare the data:\n",
    "- Multi-hot encode lists from `train_data` and `train_labels` to turn them into vectors of 0s and 1s. This would mean, for instance, turning the sequence [8, 5] into a 10,000-dimensional vector that would be all 0s except for indices 8 and 5, which would be 1s. Then you could use a Dense layer, capable of handling floating-point vector data, as the first layer in your model.\n",
    "- Change data type in `test_data` and `test_labels` from `int64` into `float32`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a919e12",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "Build your model. Take into consideration that the input data is vectors, and the labels are scalars (1s and 0s) and a type of model that performs well on such a problem is a plain stack of densely connected (Dense) layers with relu activations. Think about:\n",
    "- How many layers to use?\n",
    "- How many units to choose for each layer?\n",
    "\n",
    "Compile your model choosing a proper optimizer, loss function, and metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b40e5",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "Validate your model: \n",
    "- Create a validation set by setting apart 10,000 samples from the original training data.\n",
    "- Train the model for 20 epochs in mini-batches of 512 samples from training data.\n",
    "- Monitor loss and accuracy on the 10,000 samples from the validation set.\n",
    "- Make a plot of the training and validation loss.\n",
    "</br><img src=2.png/>\n",
    "- Make a plot of the training and validation accuracy. \n",
    "</br><img src=3.png/>\n",
    "- Choose a proper number of epochs to train the model on the entire train data to prevent overfitting, and then evaluate it on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a99212d",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "Do the following experiments:\n",
    "- Try using less or more representation layers, and see how doing so affects validation and test accuracy.\n",
    "- Try using layers with more units or fewer units.\n",
    "- Try using the `mse` loss function instead of `binary_crossentropy`.\n",
    "- Try using the `tanh` activation instead of `relu`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
